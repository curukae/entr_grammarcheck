{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df34da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Egemen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Egemen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Egemen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "TurkishMorphology instance initialized in 15.337981462478638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-06 14:18:02,789 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 15.337981462478638\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Egemen\\mambaforge\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed analysis saved to C:\\Users\\Egemen\\Desktop\\diss\\Diss Stuff\\[2] Pretest\\detailed_analysis.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "import re \n",
    "import language_tool_python\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from zemberek import TurkishMorphology, TurkishSentenceNormalizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import openpyxl\n",
    "\n",
    "# NLTK utils\n",
    "nltk.download('punkt')  \n",
    "nltk.download('words')  \n",
    "nltk.download('names') \n",
    "\n",
    "\n",
    "# Load English words and common names from NLTK\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "english_names = set(nltk.corpus.names.words())\n",
    "\n",
    "# Initialize TurkishMorphology and TurkishSentenceNormalizer\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "normalizer = TurkishSentenceNormalizer(morphology)\n",
    "\n",
    "# Function to correct codec awkwardness\n",
    "def correct_codec_awardness(text):\n",
    "    replacements = {\n",
    "        'Å': 'S',  # Ş -> S\n",
    "        'Å': 's',  # ş -> s\n",
    "        'Ä±': 'i',  # ı -> i\n",
    "        'Ä': 'g',  # ğ -> g\n",
    "        'Ã¼': 'u',  # ü -> u\n",
    "        'Ã¶': 'o',  # ö -> o\n",
    "        'Ã§': 'c',  # ç -> c\n",
    "        'Ä': 'G',  # Ğ -> G\n",
    "        'Å¸': 'Y',  # Ÿ -> Y\n",
    "        'Ä': 'c',  # ć -> c\n",
    "        'Ã': 'O',  # Ö -> O\n",
    "        'Ã': 'C',  # Ç -> C\n",
    "        'Ã': 'U',  # Ü -> U\n",
    "        'â': \"'\",   # ’ -> '\n",
    "        'â': '“',  # “ -> “\n",
    "        'â': '”',  # ” -> ”\n",
    "        'â': '–',  # – -> –\n",
    "        'â': '—',  # — -> —\n",
    "        'â¦': '…',  # … -> …\n",
    "        'â¬': '€',  # € -> €\n",
    "    }\n",
    "    \n",
    "    for wrong, right in replacements.items():\n",
    "        text = text.replace(wrong, right)\n",
    "    return text\n",
    "\n",
    "# Function to delete words containing weird Turkish characters\n",
    "def delete_weird_turkish_words(text):\n",
    "    weird_turkish_chars = r'[Å|Ä|Ã|||||]'\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if not re.search(weird_turkish_chars, word)]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Function to convert Turkish characters to English counterparts\n",
    "def convert_turkish_characters(text):\n",
    "    turkish_to_english = {\n",
    "        'ü': 'u',\n",
    "        'ö': 'o',\n",
    "        'ş': 's',\n",
    "        'ç': 'c',\n",
    "        'ğ': 'g',\n",
    "        'ı': 'i',\n",
    "        'İ': 'I',\n",
    "        'Ü': 'U',\n",
    "        'Ö': 'O',\n",
    "        'Ş': 'S',\n",
    "        'Ç': 'C',\n",
    "        'Ğ': 'G'\n",
    "    }\n",
    "    \n",
    "    for turkish_char, english_char in turkish_to_english.items():\n",
    "        text = text.replace(turkish_char, english_char)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# RE function to remove names from the corpus\n",
    "def remove_names(text, name_list):\n",
    "    pattern = r'\\b(' + '|'.join(re.escape(name) for name in name_list) + r')\\b'\n",
    "    cleaned_text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "# CleanSweep 2.0 Function\n",
    "def clean_sweep_2_0(text):\n",
    "    # Step 1: Replace lines with reviewer comments\n",
    "    step1_pattern = r\"^\\s*$\\r?\\n(?=^\\[[a-zA-Z]\\][^\\n\\r]*)\"\n",
    "    text = re.sub(step1_pattern, r\"\\r\\n\\r\\n$Reviewer Comments$\\r\\n\", text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Step 2: Remove everything after $Reviewer Comments$\n",
    "    step2_pattern = r\"\\$Reviewer Comments\\$(?s).*$\"\n",
    "    text = re.sub(step2_pattern, '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Step 3: Replace remaining annotations [a], [b], etc., with a single whitespace\n",
    "    step3_pattern = r\"\\[[a-zA-Z]\\]\"\n",
    "    text = re.sub(step3_pattern, ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# PREPROCESSING\n",
    "def preprocess_corpus(text, name_list):\n",
    "    # Step 1: Apply CleanSweep 2.0 procedure\n",
    "    text = clean_sweep_2_0(text)\n",
    "    \n",
    "    # Step 2: Correct codec awkwardness\n",
    "    text = correct_codec_awardness(text)\n",
    "    \n",
    "    # Step 3: Delete words containing weird Turkish characters\n",
    "    text = delete_weird_turkish_words(text)\n",
    "    \n",
    "    # Step 4: Convert Turkish characters to English counterparts\n",
    "    text = convert_turkish_characters(text)\n",
    "    \n",
    "    # Step 5: Remove names from the corpus\n",
    "    text = remove_names(text, name_list)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# LANGTOOL BEGINS HERE - NO WHITESPACE TYPOGRAPHY OR LOCALE VIOLATION\n",
    "def count_grammatical_errors(tool, text):\n",
    "    matches = tool.check(text)\n",
    "    filtered_matches = [\n",
    "        match for match in matches\n",
    "        if match.ruleIssueType not in ['whitespace', 'typographical', 'locale-violation'] and\n",
    "           not \"is British English\" in match.message and\n",
    "           not match.context[match.offsetInContext:match.offsetInContext + match.errorLength].strip().lower() in ['turkiye', 'türkiye']\n",
    "    ]\n",
    "    return len(filtered_matches), filtered_matches\n",
    "\n",
    "# Function to count words in the text\n",
    "def count_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(words), words\n",
    "\n",
    "# Function to check if a word is Turkish using Zemberek\n",
    "def is_turkish(word):\n",
    "    try:\n",
    "        normalized_word = normalizer.normalize(word)\n",
    "        analysis = morphology.analyze(normalized_word)\n",
    "        for result in analysis:\n",
    "            if not result.is_unknown():\n",
    "                return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing word '{word}': {e}\")\n",
    "    return False\n",
    "\n",
    "# Function to extract the error word(s) from a match\n",
    "def extract_error_word(match):\n",
    "    context = match.context\n",
    "    error_start = match.offsetInContext\n",
    "    error_end = error_start + match.errorLength\n",
    "    error_word = context[error_start:error_end].strip()\n",
    "    \n",
    "    # If the error word contains a hyphen, split it into separate words\n",
    "    if '-' in error_word:\n",
    "        return error_word.split('-')\n",
    "    return [error_word]\n",
    "\n",
    "# Function to process all text files in the directory with preprocessing and analysis\n",
    "def process_corpus_directory_with_analysis(directory, name_list):\n",
    "    tool = language_tool_python.LanguageTool('en-US')\n",
    "    \n",
    "    # Summary data and error details will be stored in pandas dataframes\n",
    "    summary_data = []\n",
    "    error_details_list = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='iso-8859-9') as file:\n",
    "                text = file.read()\n",
    "\n",
    "                # Preprocess the text\n",
    "                text = preprocess_corpus(text, name_list)\n",
    "\n",
    "                # Perform grammatical error analysis\n",
    "                num_errors, matches = count_grammatical_errors(tool, text)\n",
    "                num_words, words = count_words(text)\n",
    "                error_to_word_ratio = num_errors / num_words if num_words > 0 else 0\n",
    "\n",
    "                # Store the summary data for each file\n",
    "                summary_data.append({\n",
    "                    'Filename': filename,\n",
    "                    'NumErrors': num_errors,\n",
    "                    'NumWords': num_words,\n",
    "                    'ErrorToWordRatio': error_to_word_ratio\n",
    "                })\n",
    "\n",
    "                # Store error details for each match\n",
    "                for match in matches:\n",
    "                    error_words = extract_error_word(match)\n",
    "                    if not error_words:\n",
    "                        error_words = [\"[Unable to extract]\"]\n",
    "                    for error_word in error_words:\n",
    "                        error_details_list.append({\n",
    "                            'Filename': filename,\n",
    "                            'Sentence': match.sentence,\n",
    "                            'Error': error_word,\n",
    "                            'Category': match.ruleIssueType,\n",
    "                            'Message': match.message\n",
    "                        })\n",
    "\n",
    "    # Convert to pandas DataFrames\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    error_details_df = pd.DataFrame(error_details_list)\n",
    "\n",
    "    # Manipulate the data: Remove rows where the error word is Turkish\n",
    "    updated_summary = {}\n",
    "\n",
    "    for filename in summary_df['Filename']:\n",
    "        df_file_errors = error_details_df[error_details_df['Filename'] == filename]\n",
    "        num_words = summary_df.loc[summary_df['Filename'] == filename, 'NumWords'].values[0]\n",
    "        \n",
    "        rows_to_drop = df_file_errors[df_file_errors['Error'].apply(is_turkish)].index\n",
    "        df_file_errors.drop(rows_to_drop, inplace=True)\n",
    "\n",
    "        # Calculate updated NumErrors and ErrorToWordRatio\n",
    "        updated_num_errors = df_file_errors.shape[0]\n",
    "        updated_error_to_word_ratio = updated_num_errors / num_words if num_words > 0 else 0\n",
    "        \n",
    "        # Store the updated NumErrors and ErrorToWordRatio\n",
    "        updated_summary[filename] = {\n",
    "            'NumErrors': updated_num_errors,\n",
    "            'ErrorToWordRatio': updated_error_to_word_ratio\n",
    "        }\n",
    "\n",
    "        # Replace the error details for the file in the main dataframe\n",
    "        error_details_df = error_details_df.drop(error_details_df[error_details_df['Filename'] == filename].index)\n",
    "        error_details_df = pd.concat([error_details_df, df_file_errors])\n",
    "\n",
    "    # Update the summary DataFrame\n",
    "    for filename, data in updated_summary.items():\n",
    "        summary_df.loc[summary_df['Filename'] == filename, 'NumErrors'] = data['NumErrors']\n",
    "        summary_df.loc[summary_df['Filename'] == filename, 'ErrorToWordRatio'] = data['ErrorToWordRatio']\n",
    "\n",
    "    # Save to Excel\n",
    "    output_excel = os.path.join(directory, 'detailed_analysis.xlsx')\n",
    "    with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:\n",
    "        summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "        error_details_df.to_excel(writer, sheet_name=\"Error Details\", index=False)\n",
    "\n",
    "    print(f\"Detailed analysis saved to {output_excel}\")\n",
    "\n",
    "# PATH\n",
    "folder_path = r'C:\\Users\\Egemen\\Desktop\\diss\\Diss Stuff\\[2] Pretest'\n",
    "name_list = ['aleyna kaymak', 'nida alışık', 'şenay tatlı', 'aleyna kaymak', 'helin tals', 'abidin yıldırım', 'emine kahraman', 'sümeyye çakmak', 'elif okumuş', 'özlem doğan', 'emine kahraman', 'eren avcı', 'osman şen', 'kübra değirmenci', 'kübra köse', 'sura ural', 'hicran helin talş', 'melis eren', 'ceren erkek', 'dilara', 'mehmetali andaç beylikçi', 'nupelda kandemir', 'sanem İzci', 'zehra esmer', 'ayşen köse', 'demet toprak', 'fadime değirmenci', 'bilge nur ezmez', 'celal samyurek', 'ezgi samyurek', 'yusuf buğra kılıç', 'İrem kaya', 'nagehan karacaoğlu', 'hicran helin talş', 'hatice çelen', 'berfin yelken', 'umay kuşcu', 'abidin yıldırım', 'öykü çepelli', 'cahit mert tümen', 'semih çalışkan', 'beyza bayrak', 'sıla lük', 'eda nur yardım', 'fatma ördek', 'nida akkuş', 'merve er', 'ayşe doğan', 'büşra bacak', 'merve er', 'özlem doğan', 'burak şahin', 'nilsu zeren', 'fatih hasoğlu', 'zahide kara', 'mihriban koç', 'ecem ceren çevik', 'sıla ercan', 'dilara gülbağlar', 'ayşe sapmaz', 'halime oğurer', 'melis üçkan', 'buse azgın', 'merve eroğlu', 'emircan parlak', 'hilal şeker', 'selin keskin', 'büşra yücel', 'turancan çelikay', 'özgenur kısa', 'mustafa güloğlu', 'öznur kaya', 'ecem ceren çevik', 'elanur günüç', 'aleyna boy', 'deniz ortaçbayram', 'havva aydın', 'tuğçe özdemir', 'songül begit', 'esma yerebakan', 'ece berfu kaya', 'özge erkaya', 'ülkü sahra karaca', 'sema karasu', 'dilay ada', 'zehra öztürk', 'kardelen avcı', 'halil umut tez', 'halil umut tez', 'hasan mert menteş', 'selin yücelbulut', 'süeda öntürkler', 'egemen curuk', 'arın çağla kırmızıçiçek', 'engin can yayla', 'aleyna kaymak', 'nida alisik', 'senay tatli', 'aleyna kaymak', 'abidin yildirim', 'emine kahraman', 'sumeyye cakmak', 'elif okumus', 'ozlem dogan', 'emine kahraman', 'eren avci', 'osman sen', 'kubra degirmenci', 'kubra kose', 'sura ural', 'hicran helin talsh', 'melis eren', 'ceren erkek', 'dilara', 'mehmetali andac beylikci', 'nupelda kandemir', 'sanem Izci', 'zehra esmer', 'ayshen kose', 'demet toprak', 'fadime degirmenci', 'bilge nur ezmez', 'celal samyurek', 'ezgi samyurek', 'yusuf bugra kilic', 'Irem kaya', 'nagehan karacaoglu', 'hicran helin talsh', 'hatice celen', 'berfin yelken', 'umay kuscu', 'abidin yildirim', 'oyku cepelli', 'cahit mert tuman', 'semih caliskan', 'beyza bayrak', 'sila luk', 'eda nur yardim', 'fatma ordek', 'nida akkus', 'merve er', 'ayse dogan', 'busra bacak', 'merve er', 'ozlem dogan', 'burak sahin', 'nilsu zeren', 'fatih hasoglu', 'zahide kara', 'mihriban koc', 'ecem ceren cevik', 'sila ercan', 'dilara gulbaglar', 'ayse sapmaz', 'halime ogurer', 'melis uckan', 'buse azgin', 'merve eroglu', 'emircan parlak', 'hilal seker', 'selin keskin', 'busra yucel', 'turancan celikay', 'ozgenur kisa', 'mustafa guloglu', 'oznur kaya', 'ecem ceren cevik', 'elanur gunuc', 'aleyna boy', 'deniz ortacbayram', 'havva aydin', 'tugce ozdemir', 'songul begit', 'esma yerebakan', 'ece berfu kaya', 'ozge erkaya', 'ulku sahra karaca', 'sema karasu', 'dilay ada', 'zehra ozturk', 'kardelen avci', 'halil umut tez', 'halil umut tez', 'hasan mert mentes', 'selin yucelbulut', 'sueda onturkl', 'egemen curuk', 'arin cagla kirmizicicek', 'engin can yayla', 'aleyna', 'nida', 'senay', 'emine', 'sumeyye', 'elif', 'ozlem', 'eren', 'osman', 'sura', 'hicran', 'melis', 'ceren', 'dilara', 'mehmetali', 'nupelda', 'sanem', 'zehra', 'ayshen', 'demet', 'fadime', 'bilge', 'celal', 'ezgi', 'yusuf', 'Irem', 'nagehan', 'hatice', 'berfin', 'umay', 'oyku', 'cahit', 'semih', 'beyza', 'eda', 'fatma', 'nida', 'merve', 'ayse', 'busra', 'burak', 'nilsu', 'fatih', 'zahide', 'mihriban', 'ecem', 'sila', 'halime', 'buse', 'emircan', 'hilal', 'selin', 'turancan', 'ozgenur', 'mustafa', 'oznur', 'elanur', 'deniz', 'havva', 'tugce', 'songul', 'esma', 'ece', 'ozge', 'ulku', 'sema', 'dilay', 'kardelen', 'halil', 'hasan', 'sueda', 'arin', 'engin', 'kaymak', 'alisik', 'tatli', 'yildirim', 'kahraman', 'cakmak', 'okumus', 'dogan', 'sen', 'degirmenci', 'kose', 'ural', 'talsh', 'erkek', 'beylikci', 'kandemir', 'Izci', 'esmer', 'kose', 'toprak', 'ezmez', 'samyurek', 'kilic', 'karacaoglu', 'celen', 'yelken', 'kuscu', 'cepelli', 'tuman', 'caliskan', 'bayrak', 'luk', 'yardim', 'ordek', 'akkus', 'sahin', 'zeren', 'hasoglu', 'kara', 'koc', 'cevik', 'ercan', 'gulbaglar', 'sapmaz', 'ogurer', 'uckan', 'azgin', 'eroglu', 'parlak', 'seker', 'keskin', 'yucel', 'celikay', 'kisa', 'guloglu', 'kaya', 'gunuc', 'ortacbayram', 'aydin', 'ozdemir', 'begit', 'yerebakan', 'erkaya', 'karaca', 'karasu', 'ada', 'ozturk', 'avci', 'tez', 'mentes', 'yucelbulut', 'onturkl', 'curuk', 'kirmizicicek', 'yayla', 'mehmetali', 'nupelda', 'sanem', 'ayshen', 'ayse', 'eda', 'burak', 'nilsu', 'mihriban', 'ecem', 'sila', 'halime', 'buse', 'emircan', 'engin', 'arin', 'egemen', 'curuk']\n",
    "process_corpus_directory_with_analysis(folder_path, name_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b917953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
